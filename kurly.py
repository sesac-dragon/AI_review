import sys
import io

# Windows í™˜ê²½ì˜ subprocessì—ì„œ í•œê¸€ ì¶œë ¥ì´ ê¹¨ì§€ëŠ” í˜„ìƒ ë°©ì§€
sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')

# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤ì„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤
from dotenv import load_dotenv
import os, re, time, hashlib, json
from dataclasses import dataclass
from typing import List, Optional, Set
from datetime import datetime
import argparse

# ì›¹ ë¸Œë¼ìš°ì € ì œì–´ë¥¼ ìœ„í•œ Selenium ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤ì„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException
from webdriver_manager.chrome import ChromeDriverManager

# .env íŒŒì¼ì—ì„œ í™˜ê²½ ë³€ìˆ˜ë¥¼ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤
load_dotenv()

# í¬ë¡¤ë§í•  ìµœëŒ€ í˜ì´ì§€ ìˆ˜ë¥¼ ì •ì˜í•©ë‹ˆë‹¤
MAX_PAGES   = 30
# ì›¹ í˜ì´ì§€ ë¡œë”©ì„ ê¸°ë‹¤ë¦´ ìµœëŒ€ ì‹œê°„(ì´ˆ)ì„ ì •ì˜í•©ë‹ˆë‹¤
WAIT_SEC    = 15
# í˜ì´ì§€ ì´ë™ ë“± ì‘ì—… í›„ ì ì‹œ ê¸°ë‹¤ë¦´ ì‹œê°„(ì´ˆ)ì„ ì •ì˜í•©ë‹ˆë‹¤
SLEEP_SEC   = 2.0

# ë°ì´í„° í´ë˜ìŠ¤ë¥¼ ì‚¬ìš©í•˜ì—¬ ë¦¬ë·° ë°ì´í„°ì˜ êµ¬ì¡°ë¥¼ ì •ì˜í•©ë‹ˆë‹¤
@dataclass
class Review:
    product_name: Optional[str]
    content: str
    date: Optional[str]

# Selenium ì›¹ë“œë¼ì´ë²„ë¥¼ ì„¤ì •í•˜ê³  ìƒì„±í•˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤
def setup_driver(headless: bool = True) -> webdriver.Chrome:
    opts = Options()
    if headless:
        opts.add_argument("--headless=new")
    opts.add_argument("--disable-gpu")
    opts.add_argument("--no-sandbox")
    opts.add_argument("--window-size=1400,2200")
    opts.add_argument("--lang=ko-KR")
    opts.add_argument("--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64" \
                      "AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36")
    opts.add_experimental_option("excludeSwitches", ["enable-automation"])
    opts.add_experimental_option("useAutomationExtension", False)
    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=opts)
    driver.execute_cdp_cmd("Page.addScriptToEvaluateOnNewDocument", {
        "source": "Object.defineProperty(navigator, 'webdriver', {get: () => undefined})"
    })
    return driver

# ì—¬ëŸ¬ XPath ì¤‘ í•˜ë‚˜ë¼ë„ ë‚˜íƒ€ë‚  ë•Œê¹Œì§€ ê¸°ë‹¤ë¦¬ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤
def wait_any(driver, xpaths: list, timeout=WAIT_SEC):
    end = time.time() + timeout
    last_err = None
    while time.time() < end:
        for xp in xpaths:
            try:
                el = driver.find_element(By.XPATH, xp)
                if el: return el
            except Exception as e:
                last_err = e
        time.sleep(0.2)
    if last_err: raise TimeoutException(str(last_err))
    raise TimeoutException("wait_any timeout")

# ì›¹í˜ì´ì§€ì˜ íŒì—… ì°½ì„ ë‹«ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤
def close_popups(driver):
    for xp in [
        "//button[contains(., 'ë‹«ê¸°') or contains(., 'Close')]",
        "//button[@aria-label='ë‹«ê¸°' or @aria-label='Close']",
        "//div[@role='dialog']//button[contains(., 'ë‹«ê¸°') or contains(., 'Close')]"
    ]:
        try:
            for el in driver.find_elements(By.XPATH, xp):
                driver.execute_script("arguments[0].click();", el)
                time.sleep(0.1)
        except Exception:
            pass

# ìƒí’ˆ í˜ì´ì§€ì˜ 'í›„ê¸°' íƒ­ìœ¼ë¡œ ì´ë™í•˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤
def goto_review_tab(driver, url):
    driver.get(url + "#review")
    WebDriverWait(driver, WAIT_SEC).until(EC.presence_of_element_located((By.TAG_NAME, "body")))
    time.sleep(SLEEP_SEC)
    close_popups(driver)
    for xp in [
        "//button[contains(., 'í›„ê¸°') or contains(., 'Review')]",
        "//a[contains(., 'í›„ê¸°') or contains(., 'Review')]",
        "//nav//li[.//span[contains(., 'í›„ê¸°') or contains(., 'Review')] or contains(., 'í›„ê¸°') or contains(., 'Review')]"
    ]:
        try:
            btn = driver.find_element(By.XPATH, xp)
            driver.execute_script("arguments[0].scrollIntoView({block:'center'});", btn)
            time.sleep(0.2)
            btn.click()
            break
        except NoSuchElementException:
            continue
        except Exception:
            pass
    for _ in range(8):
        driver.execute_script("window.scrollBy(0, 800);")
        time.sleep(0.25)
    wait_any(driver, [
        "//h2[contains(., 'ìƒí’ˆ í›„ê¸°') or contains(., 'Product Review')]",
        "//*[contains(text(),'ë„ì›€ë¼ìš”') or contains(text(),'Helpful')]",
        "//section[.//h2[contains(., 'í›„ê¸°') or contains(., 'Review')]]//li"
    ], timeout=WAIT_SEC)

# í˜ì´ì§€ì—ì„œ ìƒí’ˆ ì´ë¦„ì„ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤
def get_product_name(driver) -> Optional[str]:
    for xp in ["//h1", "//header//h1", "//h2"]:
        try:
            t = driver.find_element(By.XPATH, xp).text.strip()
            if 2 < len(t) < 120:
                return t
        except Exception:
            continue
    return None

# ë¸Œë¼ìš°ì €ì—ì„œ JavaScriptë¥¼ ì‹¤í–‰í•˜ì—¬ ë¦¬ë·° ë°ì´í„°ë¥¼ íŒŒì‹±í•˜ëŠ” ì½”ë“œì…ë‹ˆë‹¤
JS_PARSE = r"""(() => {
  try {
    const header = Array.from(document.querySelectorAll('h2, h3'))
      .find(h => /ìƒí’ˆ\s*í›„ê¸°|í›„ê¸°|Review/.test(h.textContent || ''));
    if (!header) return JSON.stringify({ ok: true, cards: 0, items: [], note: 'no-header' });

    const root = header.closest('section') || header.parentElement || document;
    const nodes = root.querySelectorAll('li, article, div[role="listitem"], div[data-testid*="review"]');
    const dateRe = /(20\d{2}[-./]\d{2}[-./]\d{2})/;
    const items = [];
    for (const el of nodes) {
      const text = (el.innerText || '').trim();
      if (!text) continue;

      const lines = text.split(/\n+/).map(s => s.trim()).filter(Boolean);
      let content = '', maxLen = 0;
      for (const s of lines) {
        if (s.length <= maxLen) continue;
        if (/ë„ì›€ë¼ìš”|ë² ìŠ¤íŠ¸|ì˜µì…˜|ì„ íƒ|ì‘ì„±ì|ê´€ë¦¬ì|Helpful|Best|Option|Select|Author|Admin/.test(s)) continue;
        if (dateRe.test(s)) continue;
        content = s; maxLen = s.length;
      }

      const mDate = text.match(dateRe);
      const date = mDate ? mDate[1].replace(/[-/]/g, '.') : null;

      if (content && content.length >= 6) {
        items.push({ date, content });
      }
    }

    return JSON.stringify({ ok: true, cards: nodes.length, items });
  } catch (e) {
    return JSON.stringify({ ok: false, error: String(e) });
  }
})();"""

# í˜„ì¬ í˜ì´ì§€ì˜ ë¦¬ë·°ë“¤ì„ íŒŒì‹±í•˜ëŠ” ë©”ì¸ í•¨ìˆ˜ì…ë‹ˆë‹¤
def parse_page_reviews(driver, product_name: Optional[str]) -> List[Review]:
    data = None
    try:
        raw = driver.execute_script(JS_PARSE)
        if not raw: raise RuntimeError("execute_script returned empty")
        data = json.loads(raw) if not isinstance(raw, dict) else raw
    except Exception as e:
        print(f"[WARN] JS parse failed: {e}")
        data = None

    if not isinstance(data, dict) or not data.get("items"):
        return parse_page_reviews_fallback(driver, product_name)

    out: List[Review] = []
    for it in (data.get("items") or []):
        out.append(Review(
            product_name=product_name,
            content=preprocess_content(it.get("content") or "", product_name),
            date=it.get("date")
        ))
    return out

# JS íŒŒì‹± ì‹¤íŒ¨ ì‹œ ì‚¬ìš©ë  Selenium ê¸°ë°˜ì˜ í´ë°± íŒŒì„œ í•¨ìˆ˜ì…ë‹ˆë‹¤
def parse_page_reviews_fallback(driver, product_name: Optional[str]) -> List[Review]:
    out: List[Review] = []
    date_re = re.compile(r"(20\d{2}\.\d{2}\.\d{2})")
    sections = driver.find_elements(By.XPATH, "//section[.//h2[contains(., 'í›„ê¸°') or contains(., 'ìƒí’ˆ í›„ê¸°')]]")
    root = sections[0] if sections else driver
    items = root.find_elements(By.XPATH, ".//*[self::li or self::article or @role='listitem']")
    if not items:
        items = driver.find_elements(By.XPATH, "//*[self::li or self::article or @role='listitem']")
    for el in items:
        try:
            text = el.text.strip()
            if not text: continue
            m_date = date_re.search(text)
            date = m_date.group(1) if m_date else None
            lines = [s.strip() for s in re.split(r"\n+", text) if s.strip()]
            body_lines = []
            for s in lines:
                if any(k in s for k in ("ë„ì›€ë¼ìš”", "ë² ìŠ¤íŠ¸", "ì˜µì…˜", "ì„ íƒ", "ì‘ì„±ì", "ê´€ë¦¬ì")): continue
                if date_re.search(s): continue
                body_lines.append(s)
            body = " ".join(body_lines)
            body = preprocess_content(body, product_name)
            if len(body) < 6: continue
            out.append(Review(product_name=product_name, content=body, date=date))
        except Exception:
            continue
    print(f"[DEBUG] fallback parsed: {len(out)}")
    return out

# ì •ê·œì‹ì„ ì‚¬ìš©í•˜ì—¬ ë¶ˆí•„ìš”í•œ í…ìŠ¤íŠ¸ íŒ¨í„´ì„ ì •ì˜í•©ë‹ˆë‹¤
URL_RE       = re.compile(r"https?://\S+")
EMOJI_RE     = re.compile(r"[ğ€€-ô¿¿]", flags=re.UNICODE)
OPTION_LINE  = re.compile(r"(?m)^(ì˜µì…˜|ì„ íƒ|êµ¬ì„±|ìƒí’ˆëª…)\s*[:ï¼š].*$")
BRACKET_LINE = re.compile(r"(?m)^\s*\[[^\]]+\]\s*$")
MULTI_WS     = re.compile(r"\s+")

# ë¦¬ë·° ë‚´ìš©ì—ì„œ ë¶ˆí•„ìš”í•œ ë¶€ë¶„ì„ ì œê±°í•˜ëŠ” ì „ì²˜ë¦¬ í•¨ìˆ˜ì…ë‹ˆë‹¤
def preprocess_content(text: str, product_name: Optional[str]) -> str:
    s = text or ""
    s = URL_RE.sub("", s)
    s = EMOJI_RE.sub("", s)
    s = OPTION_LINE.sub("", s)
    s = BRACKET_LINE.sub("", s)
    if product_name:
        s = s.replace(product_name, "")
        pn_fuzzy = re.sub(r"\s+", r"\\s*", re.escape(product_name))
        s = re.sub(pn_fuzzy, "", s, flags=re.IGNORECASE)
    s = MULTI_WS.sub(" ", s).strip()
    return s

# ë¬¸ìì—´ì˜ í•´ì‹œê°’ì„ ê³„ì‚°í•˜ì—¬ ì¤‘ë³µ ì—¬ë¶€ë¥¼ íŒë‹¨í•˜ëŠ” ë° ì‚¬ìš©í•©ë‹ˆë‹¤
def digest(s: str) -> str:
    return hashlib.sha256(s.encode("utf-8")).hexdigest()

# 'ë‹¤ìŒ' í˜ì´ì§€ ë²„íŠ¼ì„ ì°¾ì•„ í´ë¦­í•˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤
def click_next(driver) -> bool:
    candidates = [
        "//button[normalize-space(.)='ë‹¤ìŒ']",
        "//a[normalize-space(.)='ë‹¤ìŒ']",
        "//*[@aria-label='ë‹¤ìŒ' or @aria-label='ë‹¤ìŒ í˜ì´ì§€']",
        "//button[.//span[contains(., 'ë‹¤ìŒ')]]",
        "//button[.//*[text()='â€º'] or .//*[text()='Â»'] or .//*[@aria-label='Next']"]
    
    for xp in candidates:
        try:
            btns = driver.find_elements(By.XPATH, xp)
            if not btns: continue
            btn = btns[0]
            if btn.get_attribute("disabled"): return False
            driver.execute_script("arguments[0].scrollIntoView({block:'center'});", btn)
            time.sleep(0.2)
            btn.click()
            return True
        except Exception:
            continue
    return False

# ì§€ì •ëœ URLì—ì„œ ì „ì²´ ë¦¬ë·°ë¥¼ í¬ë¡¤ë§í•˜ëŠ” ë©”ì¸ í•¨ìˆ˜ì…ë‹ˆë‹¤
def crawl(url: str, max_pages=MAX_PAGES) -> List[Review]:
    drv = setup_driver(headless=True)
    all_reviews: List[Review] = []
    seen: Set[str] = set()
    try:
        goto_review_tab(drv, url)
        product_name = get_product_name(drv)
        print(f"[INFO] product_name: '{product_name!r}'")
        page = 1
        while True:
            time.sleep(0.6)
            page_reviews = parse_page_reviews(drv, product_name)
            kept = 0
            for r in page_reviews:
                if not r.content or len(r.content) < 6: continue
                h = digest(r.content)
                if h in seen: continue
                seen.add(h); kept += 1; all_reviews.append(r)
            print(f"[DEBUG] page {page} parsed:{len(page_reviews)} kept:{kept} total:{len(all_reviews)}")
            if max_pages and page >= max_pages: break
            if not click_next(drv): break
            page += 1
            try:
                wait_any(drv, [
                    "//h2[contains(., 'ìƒí’ˆ í›„ê¸°') or contains(., 'í›„ê¸°')]",
                    "//section[.//h2[contains(., 'í›„ê¸°')]]//*[self::li or self::article or @role='listitem']"
                ], timeout=WAIT_SEC)
            except Exception:
                pass
    finally:
        drv.quit()
    return all_reviews

# ë‚ ì§œ í˜•ì‹ì˜ ë¬¸ìì—´ì„ datetime ê°ì²´ë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤
def parse_date_ymd(date_text: Optional[str]):
    if not date_text: return None
    for fmt in ("%Y.%m.%d","%Y-%m-%d","%Y/%m/%d"):
        try: return datetime.strptime(date_text, fmt).date()
        except: pass
    return None

# PostgreSQL ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ê°ì²´ë¥¼ ìƒì„±í•˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤
def get_pg_conn():
    import psycopg2
    # .env íŒŒì¼ì— ì •ì˜ëœ í™˜ê²½ ë³€ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ë¡œì»¬ Docker DBì— ì ‘ì†í•©ë‹ˆë‹¤
    return psycopg2.connect(
        host=os.getenv("PGHOST"),
        port=int(os.getenv("PGPORT")),
        dbname=os.getenv("PGDATABASE"),
        user=os.getenv("PGUSER"),
        password=os.getenv("PGPASSWORD"),
        client_encoding='utf8'
    )

# í¬ë¡¤ë§í•œ ë¦¬ë·° ë°ì´í„°ë¥¼ PostgreSQLì— ì €ì¥í•˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤
def save_postgres(reviews: List[Review]):
    try:
        import psycopg2
        from psycopg2.extras import execute_values
    except Exception as e:
        print(f"[WARN] psycopg2 ë¯¸ì„¤ì¹˜ë¡œ Postgres ì €ì¥ ìƒëµ: {e}")
        return
    rows = []
    for r in reviews:
        content = (r.content or "").strip()
        if not content: continue
        rows.append((r.product_name, content, parse_date_ymd(r.date), hashlib.sha256(content.encode("utf-8")).hexdigest()))
    if not rows:
        print("[INFO] Postgresì— ì €ì¥í•  í–‰ì´ ì—†ìŠµë‹ˆë‹¤.")
        return
    conn = get_pg_conn()
    try:
        with conn, conn.cursor() as cur:
            execute_values(cur, """
                INSERT INTO reviews (product_name, content, review_date, content_hash)
                VALUES %s
                ON CONFLICT (content_hash) DO NOTHING
            """, rows, page_size=1000)
        print(f"[INFO] Postgres ì €ì¥ ì™„ë£Œ: {len(rows)}ê±´ ì‹œë„(ì¤‘ë³µì€ ë¬´ì‹œ)")
    finally:
        conn.close()

# ì´ ìŠ¤í¬ë¦½íŠ¸ê°€ ì§ì ‘ ì‹¤í–‰ë  ë•Œ í˜¸ì¶œë˜ëŠ” ë©”ì¸ ë¸”ë¡ì…ë‹ˆë‹¤
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='ë§ˆì¼“ì»¬ë¦¬ ìƒí’ˆ ë¦¬ë·° í¬ë¡¤ëŸ¬')
    parser.add_argument('url', type=str, help='í¬ë¡¤ë§í•  ìƒí’ˆ í˜ì´ì§€ URL')
    parser.add_argument('--max_pages', type=int, default=MAX_PAGES, help=f'ìµœëŒ€ í¬ë¡¤ë§ í˜ì´ì§€ ìˆ˜ (ê¸°ë³¸ê°’: {MAX_PAGES})')
    args = parser.parse_args()

    reviews = crawl(args.url, max_pages=args.max_pages)
    print("Collected (after dedupe):", len(reviews))
    if reviews:
        save_postgres(reviews)
