# 컬리 VOC Analyzer: AI 기반 고객의 소리 분석 대시보드

**고객의 목소리에서 비즈니스 기회를 찾아내는 자동화 파이프라인**

<br>

## 📖 1. 프로젝트 소개

이 프로젝트는 **'고객의 소리(VOC, Voice of Customer)'** 데이터를 분석하여 비즈니스 가치를 창출하는 AI 기반 플랫폼입니다. '고객의 소리'란 리뷰, 문의, 피드백 등 고객이 기업에 남기는 모든 형태의 의견을 의미합니다. 본 플랫폼은 마켓컬리의 리뷰를 자동으로 수집하고 분석하여, 흩어져 있는 고객의 의견 속에서 제품의 강점, 약점, 그리고 새로운 성장 기회를 발견하는 것을 목표로 합니다.

## 💡 2. 프로젝트 동기: 왜 이 문제를 해결해야 했는가?

평소 마켓컬리를 자주 이용하면서, 상품 구매 결정에 큰 영향을 미치는 후기 시스템에 한 가지 아쉬운 점을 발견했습니다. 바로 **별점 시스템의 부재**와 **카테고리별 후기 분석의 어려움**이었습니다.

예를 들어, 아무리 좋은 후기가 많아도 '맛'에 대한 칭찬인지, '포장'에 대한 칭찬인지, 혹은 '배송'에 대한 칭찬인지 한눈에 파악하기 어려웠습니다. 특히 "포장은 꼼꼼했지만, 맛은 아쉬웠어요"와 같은 복합적인 후기의 경우, 긍정인지 부정인지 판단하기 모호했습니다.

이커머스 도메인에 대한 경험을 바탕으로, 저는 이것이 비단 저만의 불편함이 아니라, 많은 사용자들이 겪고 있으며 판매자 또한 고객의 정확한 피드백을 놓치게 만드는 **중요한 문제**라고 판단했습니다.

이 프로젝트는 바로 이 문제를 해결하기 위해 시작되었습니다. 흩어져 있는 텍스트 후기 데이터에 담긴 고객의 감정과 의견을 AI로 분석하고 정량화하여, **사용자에게는 현명한 구매 결정을 돕는 정보를, 판매자에게는 제품 개선을 위한 구체적인 인사이트를 제공**하고자 했습니다. 이는 단순히 코드를 작성하는 것을 넘어, 실제 문제를 발견하고 데이터를 활용해 해결책을 제시하는 전 과정을 보여줍니다.

## ✨ 3. 이 프로젝트의 기술적 강점 (Key Highlights)

-   **End-to-End 시스템 설계:** 데이터 수집부터 분석, 저장, 시각화에 이르는 전체 파이프라인을 직접 설계하고 구축했습니다.
-   **LLM의 안정적 활용:** `Pydantic` 모델로 LLM의 출력 스키마를 강제하여, 비정형 분석 결과를 신뢰도 높은 정형 데이터로 변환하는 엔지니어링 문제를 해결했습니다.
-   **안정성과 확장성을 고려한 인프라:** `Docker` 기반의 컨테이너 환경과 `PostgreSQL`을 채택하여, 재현 가능하고 확장성 있는 시스템을 구축했습니다.
-   **사용자 중심의 데이터 시각화:** `Streamlit`을 활용하여 데이터 분석가가 아니더라도 누구나 쉽게 인사이트를 탐색할 수 있는 인터랙티브 대시보드를 구현했습니다.

## ⚙️ 4. 주요 기능

-   **📊 인터랙티브 대시보드:** `Streamlit`으로 구축된 반응형 웹 UI를 통해, 사용자는 필터링, 검색 등 다양한 상호작용으로 아래 분석 결과를 직관적으로 확인할 수 있습니다.
    -   AI가 예측한 평균 평점, 긍정/부정 비율 등 핵심 지표(KPI)
    -   워드클라우드, 막대 차트 등 다양한 시각화 자료
    -   AI가 생성한 상품의 강점, 약점, 개선 제안이 담긴 종합 리포트

-   **🕷️ 자동화된 데이터 수집 (Web Scraping):** `Selenium`을 활용하여 지정된 상품 페이지의 리뷰를 동적으로 수집하며, 수집된 리뷰의 해시(Hash) 값을 비교하여 중복 저장을 방지하고 데이터의 정합성을 유지합니다.

-   **🧠 AI 심층 분석 (Data Analysis):** `LangChain`과 `OpenAI GPT-4o` 모델을 활용하여 단순한 긍/부정 분석을 넘어 다음과 같은 깊이 있는 분석을 수행합니다.
    -   **감성 분석:** 긍정, 부정, 중립 세분화 및 감성 점수(-1.0 ~ 1.0) 부여
    -   **핵심 추출:** 리뷰의 핵심 `키워드`와 `카테고리`(맛, 가격, 배송 등) 자동 분류
    -   **인사이트 도출:** 리뷰 내용을 바탕으로 `AI 예측 별점`, `사용자 페르소나`, `제품/서비스 개선 제안` 등 실용적인 정보 생성

## 🌊 5. 아키텍처 및 프로세스 흐름

이 프로젝트는 명확한 데이터 파이프라인을 따라 동작합니다.

**Step 1: 데이터 수집 (Scraping)**
> `👨‍💻 사용자` → `🖥️ 대시보드` → `🕷️ kurly.py`

사용자가 대시보드에서 URL을 입력하고 수집을 시작하면, `Selenium` 스크래퍼가 리뷰 데이터를 수집합니다.

**Step 2: 저장 및 중복 제거 (Storage)**
> `🕷️ kurly.py` → `🗄️ PostgreSQL`

수집된 데이터는 중복 검사를 거쳐 `PostgreSQL` 데이터베이스에 안전하게 저장됩니다.

**Step 3: AI 심층 분석 (Analysis)**
> `👨‍💻 사용자` → `🖥️ 대시보드` → `🧠 analyze.py` → `🤖 OpenAI` → `🗄️ PostgreSQL`

사용자가 분석을 요청하면, `LangChain`과 `GPT-4o`가 DB의 리뷰를 분석하여 추출된 인사이트를 다시 DB에 저장합니다.

**Step 4: 시각화 (Visualization)**
> `🗄️ PostgreSQL` → `🖥️ 대시보드` → `👨‍💻 사용자`

대시보드는 최종 분석 데이터를 DB에서 조회하여, 사용자가 한눈에 이해할 수 있도록 다양한 차트와 리포트로 시각화합니다.

## 🛠️ 6. 기술 스택 (Tech Stack)

| 구분 | 기술 |
| :--- | :--- |
| **Backend** | Python |
| **AI & NLP** | LangChain, OpenAI API (GPT-4o) |
| **Database** | PostgreSQL |
| **Web Frontend** | Streamlit |
| **Infrastructure** | Docker, Docker Compose |
| **Web Scraping** | Selenium, WebDriver Manager |
| **Data & Visualization** | Pandas, Plotly, Matplotlib, ECharts |

## 🚀 7. 시작하기 (Getting Started)

1.  **사전 준비:** `Docker Desktop`을 설치하고 `OpenAI API 키`를 발급받습니다.
2.  **리포지토리 클론:** `git clone <REPOSITORY_URL>`
3.  **환경 변수 설정:** 프로젝트 루트에 `.env` 파일을 생성하고 아래 내용을 채웁니다.
    ```env
    PGHOST=db
    PGPORT=5432
    PGDATABASE=kurlydb
    PGUSER=kurlyuser
    PGPASSWORD=kurlypassword
    OPENAI_API_KEY="YOUR_OPENAI_API_KEY_HERE"
    ```
4.  **실행:** 터미널에서 `docker-compose up -d --build` 명령어를 입력합니다.
5.  **접속:** 웹 브라우저에서 `http://localhost:8501`로 접속합니다.
